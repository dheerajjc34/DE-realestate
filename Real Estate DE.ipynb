{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7461bc3-ad39-494f-90eb-d91d11f910bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting homeharvest\n  Downloading homeharvest-0.3.33-py3-none-any.whl (17 kB)\nCollecting pydantic<3.0.0,>=2.7.4\n  Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 423.9/423.9 kB 8.9 MB/s eta 0:00:00\nCollecting requests<3.0.0,>=2.31.0\n  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.9/64.9 kB 7.7 MB/s eta 0:00:00\nCollecting pandas<3.0.0,>=2.1.1\n  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 80.6 MB/s eta 0:00:00\nRequirement already satisfied: python-dateutil>=2.8.2 in /databricks/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.1->homeharvest) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.1->homeharvest) (2022.7)\nCollecting tzdata>=2022.7\n  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 345.4/345.4 kB 26.5 MB/s eta 0:00:00\nRequirement already satisfied: numpy>=1.22.4 in /databricks/python3/lib/python3.10/site-packages (from pandas<3.0.0,>=2.1.1->homeharvest) (1.23.5)\nCollecting pydantic-core==2.20.1\n  Downloading pydantic_core-2.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 80.5 MB/s eta 0:00:00\nCollecting typing-extensions>=4.6.1\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting annotated-types>=0.4.0\n  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->homeharvest) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->homeharvest) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->homeharvest) (2022.12.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.31.0->homeharvest) (2.0.4)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.1.1->homeharvest) (1.16.0)\nInstalling collected packages: tzdata, typing-extensions, requests, annotated-types, pydantic-core, pandas, pydantic, homeharvest\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.4.0\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-480588a3-14d1-4438-ae35-549813a46224\n    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n  Attempting uninstall: requests\n    Found existing installation: requests 2.28.1\n    Not uninstalling requests at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-480588a3-14d1-4438-ae35-549813a46224\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: pandas\n    Found existing installation: pandas 1.5.3\n    Not uninstalling pandas at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-480588a3-14d1-4438-ae35-549813a46224\n    Can't uninstall 'pandas'. No files were found to uninstall.\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.6\n    Not uninstalling pydantic at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-480588a3-14d1-4438-ae35-549813a46224\n    Can't uninstall 'pydantic'. No files were found to uninstall.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatabricks-sdk 0.1.6 requires requests<2.29.0,>=2.28.1, but you have requests 2.32.3 which is incompatible.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed annotated-types-0.7.0 homeharvest-0.3.33 pandas-2.2.2 pydantic-2.8.2 pydantic-core-2.20.1 requests-2.32.3 typing-extensions-4.12.2 tzdata-2024.1\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n[notice] A new release of pip available: 22.3.1 -> 24.2\n[notice] To update, run: pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "pip install -U homeharvest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "381b6bfe-97dd-4402-86d5-1a7b78d496ee",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## **Scraping data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382c881d-0378-428a-b42f-120d0dffc451",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of properties: 4035\n                                        property_url  ...                                         alt_photos\n0  https://www.realtor.com/realestateandhomes-det...  ...  http://ap.rdcpix.com/8ba992edb20c6f21bb700f63d...\n1  https://www.realtor.com/realestateandhomes-det...  ...  http://ap.rdcpix.com/d26219b8f70df7ea20c11042a...\n2  https://www.realtor.com/realestateandhomes-det...  ...  http://ap.rdcpix.com/06cee0ee39e9771d359ef30fb...\n3  https://www.realtor.com/realestateandhomes-det...  ...  http://ap.rdcpix.com/cb251f60979988caed227a782...\n4  https://www.realtor.com/realestateandhomes-det...  ...  http://ap.rdcpix.com/e47b9b9a69ac98762cf373df9...\n\n[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "from homeharvest import scrape_property\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate filename based on current timestamp\n",
    "filename = \"REdata.csv\"\n",
    "\n",
    "properties = scrape_property(\n",
    "  location=\"New York, NY\",\n",
    "  listing_type=\"sold\",  # or (for_sale, for_rent, pending)\n",
    "  past_days=120,  # sold in last 30 days - listed in last 30 days if (for_sale, for_rent)\n",
    "\n",
    "  \n",
    "  mls_only=True,  # only fetch MLS listings\n",
    ")\n",
    "print(f\"Number of properties: {len(properties)}\")\n",
    "\n",
    "#properties.to_csv(filename, index=False)\n",
    "print(properties.head())\n",
    "\n",
    "dbutils.fs.mkdirs(\"/tmp\")\n",
    "dbfs_path = \"/dbfs/tmp/raw_file.csv\"\n",
    "properties.to_csv(dbfs_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21059672-1de0-42b6-bbe8-6c796da71a0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Storing raw data in ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ab91aad-8f63-4ecf-9248-ef38e3f14edc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.djrestores.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.djrestores.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.djrestores.dfs.core.windows.net\", \"sp=racwd&st=2024-07-29T01:55:11Z&se=2024-07-29T09:55:11Z&spr=https&sv=2022-11-02&sr=c&sig=tVJkrmF87EJiDz6Crqo0Ea4Av7Rlqp12da5Mrzni6zQ%3D\")\n",
    "\n",
    "source_path = \"/tmp/raw_file.csv\"\n",
    "adls_path = f\"abfss://rawdata@djrestores.dfs.core.windows.net/rawfile.csv\"\n",
    "\n",
    "# Move the file from DBFS to ADLS\n",
    "dbutils.fs.cp(f\"dbfs:{source_path}\", adls_path)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647f4d76-fd0d-4025-a150-b19f4698c591",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0411b7de-44cf-4b32-b6ca-a164bd97c937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        property_url  ... parking_garage\n0  https://www.realtor.com/realestateandhomes-det...  ...              1\n1  https://www.realtor.com/realestateandhomes-det...  ...              2\n2  https://www.realtor.com/realestateandhomes-det...  ...              0\n3  https://www.realtor.com/realestateandhomes-det...  ...              0\n4  https://www.realtor.com/realestateandhomes-det...  ...              0\n\n[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "# Drop 'agent' column and all columns to its right\n",
    "agent_index = properties.columns.get_loc('agent')\n",
    "properties = properties.iloc[:, :agent_index]\n",
    "\n",
    "#fix null values\n",
    "properties['hoa_fee'].fillna(0, inplace=True)\n",
    "properties['parking_garage'].fillna(0, inplace=True)\n",
    "\n",
    "# Export to csv\n",
    "properties.to_csv(filename, index=False)\n",
    "print(properties.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "533f6dd1-3456-4756-9581-a53941557eba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Saving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "618be924-8c0e-4365-87c8-ef91b440d3e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.djrestores.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.djrestores.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.sas.fixed.token.djrestores.dfs.core.windows.net\", \"sp=racwdlm&st=2024-07-29T02:33:14Z&se=2024-07-29T10:33:14Z&spr=https&sv=2022-11-02&sr=c&sig=DyhaRpv73pzVmZc69YTrBXhD0ygcrUZueh0WRsdlZNM%3D\")\n",
    "\n",
    "\n",
    "dbfs_path = \"/dbfs/tmp/proc_file.csv\"\n",
    "properties.to_csv(dbfs_path, index=False)\n",
    "\n",
    "source_path = \"/tmp/proc_file.csv\"\n",
    "adls_path = f\"abfss://processed@djrestores.dfs.core.windows.net/processedfile.csv\"\n",
    "\n",
    "# Move the file from DBFS to ADLS\n",
    "dbutils.fs.cp(f\"dbfs:{source_path}\", adls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85a1d91e-de1a-4101-9164-e935e0ac3c7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1187155277956403,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Real Estate DE",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
